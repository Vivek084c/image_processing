{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the files  from labels and saving to list\n",
    "path=r\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\labels\"\n",
    "list1=os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=list1[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "for i in list1:\n",
    "    item=i.split(\".\")[0]\n",
    "    temp.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the images\n",
    "img_path=r\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\"\n",
    "trg=r\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input_2\"\n",
    "c=0\n",
    "for i in temp:\n",
    "    item=i\n",
    "    shutil.copy(img_path+\"\\\\\"+item+\".jpg\",trg+\"\\\\\"+item+\".jpg\")\n",
    "    # print(i)\n",
    "    # if c==10:\n",
    "    #     break\n",
    "    # c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  ultralytics import YOLO \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\work\\\\ML\\\\image_processing\\\\YOLO V8\\\\dataset'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(7990,8000):\n",
    "#     path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "#     result=model.predict(source=path, show=True)\n",
    "#     cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7992.jpg\n"
     ]
    }
   ],
   "source": [
    "print(r\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7992.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read image and get box vectore\n",
    "path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_9995.jpg\"\n",
    "frame=cv2.imread(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_9995.jpg: 640x448 1 head, 1 body, 156.6ms\n",
      "Speed: 3.0ms preprocess, 156.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=model.predict(source=path, show=True)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Results object with attributes:\n",
       "\n",
       "boxes: ultralytics.engine.results.Boxes object\n",
       "keypoints: None\n",
       "masks: None\n",
       "names: {0: 'head', 1: 'body'}\n",
       "obb: None\n",
       "orig_img: array([[[ 33,  28,  27],\n",
       "        [ 51,  46,  45],\n",
       "        [ 59,  55,  54],\n",
       "        ...,\n",
       "        [251, 246, 243],\n",
       "        [251, 246, 243],\n",
       "        [251, 246, 243]],\n",
       "\n",
       "       [[ 31,  23,  23],\n",
       "        [ 45,  40,  39],\n",
       "        [ 54,  49,  48],\n",
       "        ...,\n",
       "        [251, 246, 243],\n",
       "        [251, 246, 243],\n",
       "        [251, 246, 243]],\n",
       "\n",
       "       [[ 37,  28,  25],\n",
       "        [ 48,  41,  38],\n",
       "        [ 56,  51,  48],\n",
       "        ...,\n",
       "        [251, 246, 243],\n",
       "        [251, 246, 243],\n",
       "        [251, 246, 243]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[147, 139, 122],\n",
       "        [149, 141, 124],\n",
       "        [133, 125, 108],\n",
       "        ...,\n",
       "        [133, 118, 102],\n",
       "        [132, 117, 101],\n",
       "        [128, 113,  97]],\n",
       "\n",
       "       [[117, 109,  92],\n",
       "        [121, 113,  96],\n",
       "        [107,  99,  82],\n",
       "        ...,\n",
       "        [128, 113,  97],\n",
       "        [130, 115,  99],\n",
       "        [135, 120, 104]],\n",
       "\n",
       "       [[138, 130, 113],\n",
       "        [134, 126, 109],\n",
       "        [109, 101,  84],\n",
       "        ...,\n",
       "        [143, 128, 112],\n",
       "        [133, 118, 102],\n",
       "        [131, 116, 100]]], dtype=uint8)\n",
       "orig_shape: (600, 400)\n",
       "path: 'D:\\\\work\\\\ML\\\\image_processing\\\\YOLO V8\\\\dataset\\\\input\\\\img_9995.jpg'\n",
       "probs: None\n",
       "save_dir: 'runs\\\\detect\\\\predict'\n",
       "speed: {'preprocess': 2.9981136322021484, 'inference': 156.5682888031006, 'postprocess': 0.9932518005371094}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([164.5233,  90.6473, 227.2965, 169.1569])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyxy=result[0].boxes.xyxy[1]\n",
    "print(xyxy)\n",
    "frame=result[0].orig_img\n",
    "frame=cv2.rectangle(frame,  (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "cv2.imshow(\"img\",frame)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_part(img,arr):\n",
    "    \"\"\"\n",
    "    img: original image \n",
    "    arr: cordinates for subdivision\n",
    "    \"\"\"\n",
    "    return img[arr[1]:arr[3],arr[0]:arr[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_0.jpg: 640x448 1 head, 1 body, 163.9ms\n",
      "Speed: 3.0ms preprocess, 163.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     232.57      43.512      308.06       121.2]\n",
      "[     182.11      123.48      336.39      577.21]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_1.jpg: 640x448 1 head, 1 body, 162.7ms\n",
      "Speed: 3.0ms preprocess, 162.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     102.51      98.725      305.54      593.65]\n",
      "[     149.38      2.1381      244.06       99.27]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_2.jpg: 640x448 1 head, 1 body, 169.7ms\n",
      "Speed: 3.0ms preprocess, 169.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     80.511           0      187.14      82.681]\n",
      "[     52.123      64.412      221.89      588.18]\n"
     ]
    }
   ],
   "source": [
    "#computing average head and body height\n",
    "head_height=0\n",
    "head_width=0\n",
    "body_height=0\n",
    "body_width=0\n",
    "\n",
    "#fucntion to get the mask of head and body for the corropnding head and body\n",
    "for i in range(0,3):\n",
    "    img_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    mask_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\Categories\\img_{i}.png\"\n",
    "    \n",
    "    result=model.predict(source=img_path, show=True)\n",
    "\n",
    "    mask_frame=cv2.imread(mask_path)\n",
    "    img_fram=cv2.imread(img_path)\n",
    "\n",
    "    #extracting corrdinates of head and body from model\n",
    "    headCordinates=np.array(result[0].boxes.cpu().numpy().xyxy[1], dtype=np.uint32)\n",
    "    bodyCosrdinates=np.array(result[0].boxes.cpu().numpy().xyxy[0], dtype=np.uint32)\n",
    "\n",
    "    print(result[0].boxes.cpu().numpy().xyxy[1])\n",
    "    print(result[0].boxes.cpu().numpy().xyxy[0])\n",
    "    \n",
    "    head_height+=headCordinates[3]-headCordinates[1]\n",
    "    head_width+=headCordinates[2]-headCordinates[0]\n",
    "\n",
    "    body_height+=bodyCosrdinates[3]-bodyCosrdinates[1]\n",
    "    body_width+=bodyCosrdinates[2]-bodyCosrdinates[0]    \n",
    "\n",
    "    #showing the head and body original image\n",
    "    cv2.imshow(\"head img\",get_img_part(img_fram,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(img_fram,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "    # #showing the head and body mask image\n",
    "    cv2.imshow(\"head img\",get_img_part(mask_frame,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(mask_frame,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.33333333333334\n",
      "128.66666666666666\n",
      "358.3333333333333\n",
      "139.33333333333334\n"
     ]
    }
   ],
   "source": [
    "print(head_height/3)\n",
    "print(head_width/3)\n",
    "print(body_height/3)\n",
    "print(body_width/3)\n",
    "\n",
    "# head->240 by 140\n",
    "# body->440 by 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.2185) tensor(-47.2631)\n"
     ]
    }
   ],
   "source": [
    "print(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"img\",result[0].orig_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     109.76      153.17      266.85      554.03]\n",
      "[     164.52      90.647       227.3      169.16]\n"
     ]
    }
   ],
   "source": [
    "for res in result:\n",
    "    a=res.boxes.cpu().numpy()\n",
    "    #getting the corrdinates of bounding box for classes\n",
    "    xyxys=a.xyxy\n",
    "    c=False\n",
    "    for xyxy in xyxys:\n",
    "        if c:\n",
    "            True\n",
    "            continue\n",
    "        print(xyxy)\n",
    "        img1=cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "    cv2.imshow(\"img_\", img1)\n",
    "    cv2.waitKey(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109 153 266 554]\n",
      "[164  90 227 169]\n"
     ]
    }
   ],
   "source": [
    "arr=np.array(xyxys[0],dtype=np.uint16)\n",
    "arr1=np.array(xyxys[1],dtype=np.uint16)\n",
    "\n",
    "print(arr)\n",
    "print(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_part(img,arr):\n",
    "    \"\"\"\n",
    "    img: original image \n",
    "    arr: cordinates for subdivision\n",
    "    \"\"\"\n",
    "    return img[arr[1]:arr[3],arr[0]:arr[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 63, 3)\n"
     ]
    }
   ],
   "source": [
    "body=frame[arr1[1]:arr1[3],arr1[0]:arr1[2]]\n",
    "head=frame[arr1[1]:arr1[3],arr1[0]:arr1[2]]\n",
    "print(head.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"temp img\",head)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"temp img\",get_img_part(frame,arr1))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temparr=np.array([\n",
    "    [1,2,3,4,5,6],\n",
    "    [7,8,9,10,11,12],\n",
    "    [13,14,15,16,17,18]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  8],\n",
       "       [13, 14]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temparr[1:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7998.jpg: 640x448 1 head, 1 body, 161.1ms\n",
      "Speed: 4.0ms preprocess, 161.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     176.63      85.494      233.31      163.06]\n",
      "[     134.03      161.07      276.26      595.02]\n",
      "(600, 400, 3)\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7999.jpg: 640x448 1 head, 1 body, 155.6ms\n",
      "Speed: 4.5ms preprocess, 155.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[      133.2      159.75      270.86      557.78]\n",
      "[     197.77      100.55      260.26       167.7]\n",
      "(600, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(7998,8000):\n",
    "    path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    original_image=cv2.imread(path)\n",
    "    result=model.predict(source=path, show=True)\n",
    "    frame=result[0].orig_img\n",
    "    a=result[0].boxes.cpu().numpy()\n",
    "    #getting the corrdinates of bounding box for classes\n",
    "    xyxys=a.xyxy\n",
    "    c=False\n",
    "    for xyxy in xyxys:\n",
    "        if c:\n",
    "            True\n",
    "            continue\n",
    "        print(xyxy)\n",
    "        img1=cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "        framed_orginal_image=cv2.rectangle(original_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "    cv2.imshow(\"img_outfile_from_NET\", img1)\n",
    "    cv2.imshow(\"img_outfile_from_original\", framed_orginal_image)\n",
    "    print(img1.shape)\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7998.jpg: 640x448 1 head, 1 body, 162.5ms\n",
      "Speed: 3.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "(600, 400, 3)\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7999.jpg: 640x448 1 head, 1 body, 163.6ms\n",
      "Speed: 5.0ms preprocess, 163.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "(600, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(7998,8000):\n",
    "    #reading the original image from path\n",
    "    path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    original_image=cv2.imread(path)\n",
    "\n",
    "    #passing the image to the model\n",
    "    result=model.predict(source=path, show=True)\n",
    "    a=result[0].boxes.cpu().numpy()\n",
    "\n",
    "    #getting the corrdinates of bounding box for classes\n",
    "    xyxys=a.xyxy\n",
    "\n",
    "    #building the bounding box\n",
    "    for xyxy in xyxys:\n",
    "        framed_orginal_image=cv2.rectangle(original_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "\n",
    "    cv2.imshow(\"img_outfile_from_original\", framed_orginal_image)\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_0.jpg: 640x448 1 head, 1 body, 155.6ms\n",
      "Speed: 3.0ms preprocess, 155.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_1.jpg: 640x448 1 head, 1 body, 159.9ms\n",
      "Speed: 2.0ms preprocess, 159.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    }
   ],
   "source": [
    "# fuction to get original image preditciton and its corrosponding head and body img\n",
    "for i in range(0,2):\n",
    "    path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    result=model.predict(source=path, show=True)\n",
    "    frame=cv2.imread(path)\n",
    "    headCordinates=np.array(result[0].boxes.cpu().numpy().xyxy[1], dtype=np.uint32)\n",
    "    bodyCosrdinates=np.array(result[0].boxes.cpu().numpy().xyxy[0], dtype=np.uint32)\n",
    "    cv2.imshow(\"head img\",get_img_part(frame,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(frame,bodyCosrdinates))\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_0.jpg: 640x448 1 head, 1 body, 178.0ms\n",
      "Speed: 4.0ms preprocess, 178.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     232.57      43.512      308.06       121.2]\n",
      "[     182.11      123.48      336.39      577.21]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_1.jpg: 640x448 1 head, 1 body, 169.1ms\n",
      "Speed: 3.0ms preprocess, 169.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     102.51      98.725      305.54      593.65]\n",
      "[     149.38      2.1381      244.06       99.27]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_2.jpg: 640x448 1 head, 1 body, 164.6ms\n",
      "Speed: 3.0ms preprocess, 164.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     80.511           0      187.14      82.681]\n",
      "[     52.123      64.412      221.89      588.18]\n"
     ]
    }
   ],
   "source": [
    "#fucntion to get the mask of head and body for the corropnding head and body\n",
    "for i in range(0,3):\n",
    "    img_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    mask_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\Categories\\img_{i}.png\"\n",
    "    \n",
    "    result=model.predict(source=img_path, show=True)\n",
    "\n",
    "    mask_frame=cv2.imread(mask_path)\n",
    "    img_fram=cv2.imread(img_path)\n",
    "\n",
    "    #extracting corrdinates of head and body from model\n",
    "    headCordinates=np.array(result[0].boxes.cpu().numpy().xyxy[1], dtype=np.uint32)\n",
    "    bodyCosrdinates=np.array(result[0].boxes.cpu().numpy().xyxy[0], dtype=np.uint32)\n",
    "\n",
    "\n",
    "    #showing the head and body original image\n",
    "    cv2.imshow(\"head img\",get_img_part(img_fram,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(img_fram,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "    # #showing the head and body mask image\n",
    "    cv2.imshow(\"head img\",get_img_part(mask_frame,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(mask_frame,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head parsing network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image form above model will be resized to (480, ,280 , 3)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def fcn8(input_shape=(320, 192, 3), num_classes=21):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Fully Convolutional Layers (FC Layers)\n",
    "    x = tf.keras.layers.Conv2D(4096, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Conv2D(4096, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Classification layer (output layer)\n",
    "    x = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax', padding='same')(x)\n",
    "\n",
    "    # Upsampling for FCN-8\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(16, 16), strides=(8, 8), padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "model = fcn8(input_shape=(320, 192, 3), num_classes=21)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body parsing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fcn8(input_shape=(640, 384, 3), num_classes=21):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Fully Convolutional Layers (FC Layers)\n",
    "    x = tf.keras.layers.Conv2D(4096, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Conv2D(4096, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Classification layer (output layer)\n",
    "    x = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax', padding='same')(x)\n",
    "\n",
    "    # Upsampling for FCN-8\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(16, 16), strides=(8, 8), padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "model = fcn8(input_shape=(640, 384, 3), num_classes=21)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your FCN-8 model (assuming you've already defined `fcn8` function as provided earlier)\n",
    "model = fcn8(input_shape=(640, 384, 3), num_classes=21)\n",
    "\n",
    "# Define your loss function (e.g., categorical crossentropy for multi-class segmentation)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define optimizer with momentum and weight decay\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9, decay=0.0005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Prepare dummy data (replace with your actual data loading and preprocessing)\n",
    "import numpy as np\n",
    "x_train = np.random.randn(100, 640, 384, 3)  # example random input data\n",
    "y_train = np.random.randint(0, 21, size=(100, 640, 384))  # example random labels\n",
    "\n",
    "# Dummy training loop (replace with your actual training loop)\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, logits)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # Print batch loss (optional)\n",
    "        print(f\"  Batch {i//batch_size+1}/{len(x_train)//batch_size}, Loss: {loss_value.numpy():.4f}\")\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "# Replace x_test and y_test with your test data\n",
    "# loss, accuracy = model.evaluate(x_test, y_test)\n",
    "# print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
